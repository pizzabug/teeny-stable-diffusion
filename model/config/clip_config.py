from dataclasses import dataclass
from typing import List, Any, TypeVar, Callable, Type, cast

from utils.typing import from_str, from_list, from_int, from_float, to_float, to_class

@dataclass
class CLIPConfig:
    """
        Configuration for CLIPTextModel.
        Generated by Quicktype.
    """
    architectures: List[str]
    attention_dropout: int
    bos_token_id: int
    dropout: int
    eos_token_id: int
    hidden_act: str
    hidden_size: int
    initializer_factor: int
    initializer_range: float
    intermediate_size: int
    layer_norm_eps: float
    max_position_embeddings: int
    model_type: str
    num_attention_heads: int
    num_hidden_layers: int
    pad_token_id: int
    projection_dim: int
    torch_dtype: str
    transformers_version: str
    vocab_size: int

    @staticmethod
    def from_dict(obj: Any) -> 'CLIPConfig':
        assert isinstance(obj, dict)
        architectures = from_list(from_str, obj.get("architectures"))
        attention_dropout = from_int(obj.get("attention_dropout"))
        bos_token_id = from_int(obj.get("bos_token_id"))
        dropout = from_int(obj.get("dropout"))
        eos_token_id = from_int(obj.get("eos_token_id"))
        hidden_act = from_str(obj.get("hidden_act"))
        hidden_size = from_int(obj.get("hidden_size"))
        initializer_factor = from_int(obj.get("initializer_factor"))
        initializer_range = from_float(obj.get("initializer_range"))
        intermediate_size = from_int(obj.get("intermediate_size"))
        layer_norm_eps = from_float(obj.get("layer_norm_eps"))
        max_position_embeddings = from_int(obj.get("max_position_embeddings"))
        model_type = from_str(obj.get("model_type"))
        num_attention_heads = from_int(obj.get("num_attention_heads"))
        num_hidden_layers = from_int(obj.get("num_hidden_layers"))
        pad_token_id = from_int(obj.get("pad_token_id"))
        projection_dim = from_int(obj.get("projection_dim"))
        torch_dtype = from_str(obj.get("torch_dtype"))
        transformers_version = from_str(obj.get("transformers_version"))
        vocab_size = from_int(obj.get("vocab_size"))
        return CLIPConfig(architectures, attention_dropout, bos_token_id, dropout, eos_token_id, hidden_act, hidden_size, initializer_factor, initializer_range, intermediate_size, layer_norm_eps, max_position_embeddings, model_type, num_attention_heads, num_hidden_layers, pad_token_id, projection_dim, torch_dtype, transformers_version, vocab_size)

    def to_dict(self) -> dict:
        result: dict = {}
        result["architectures"] = from_list(from_str, self.architectures)
        result["attention_dropout"] = from_int(self.attention_dropout)
        result["bos_token_id"] = from_int(self.bos_token_id)
        result["dropout"] = from_int(self.dropout)
        result["eos_token_id"] = from_int(self.eos_token_id)
        result["hidden_act"] = from_str(self.hidden_act)
        result["hidden_size"] = from_int(self.hidden_size)
        result["initializer_factor"] = from_int(self.initializer_factor)
        result["initializer_range"] = to_float(self.initializer_range)
        result["intermediate_size"] = from_int(self.intermediate_size)
        result["layer_norm_eps"] = to_float(self.layer_norm_eps)
        result["max_position_embeddings"] = from_int(self.max_position_embeddings)
        result["model_type"] = from_str(self.model_type)
        result["num_attention_heads"] = from_int(self.num_attention_heads)
        result["num_hidden_layers"] = from_int(self.num_hidden_layers)
        result["pad_token_id"] = from_int(self.pad_token_id)
        result["projection_dim"] = from_int(self.projection_dim)
        result["torch_dtype"] = from_str(self.torch_dtype)
        result["transformers_version"] = from_str(self.transformers_version)
        result["vocab_size"] = from_int(self.vocab_size)
        return result


def clip_config_from_dict(s: Any) -> CLIPConfig:
    return CLIPConfig.from_dict(s)


def clip_config_to_dict(x: CLIPConfig) -> Any:
    return to_class(CLIPConfig, x)
